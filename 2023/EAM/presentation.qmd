---
title: "Towards a standardized evaluation <br> of imputation methodology"
subtitle: "Potential pitfalls in simulation studies and a proposed course of action"
author: "H.I. Oberman & G. Vink"
format:
  revealjs:
    logo: logo.png
    footer: "Hanne Oberman"
    incremental: true
css: style.css
---

# 

## Missing data are ubiquituous

- missing data problems may be solved by imputing missing values
- imputation methods are evaluated through simulation studies
- there is no consensus on the evaluation of imputation methodology

## Lack of common ground

- lack of common ground in evaluating imputation methodology may lead to suboptimal use in practice
- to demonstrate the need for standardization, we highlight a set of possible pitfalls
- bringing forth a chain of potential problems in the objective assessment of the performance of imputation routines
- we suggest a (non-exhaustive) course of action for simulating and evaluating missing data problems
- and propose a move toward a standardized evaluation of imputation methodology

<!-- Developing new imputation methodology has become a very active field. Unfortunately, there is no consensus on how to perform simulation studies to evaluate the properties of imputation methods. In part, this may be due to different aims between fields and studies. For example, when evaluating imputation techniques aimed at prediction, different aims may be formulated than when statistical inference is of interest. The lack of consensus may also stem from different personal preferences or scientific backgrounds. All in all, the lack of common ground in evaluating imputation methodology may lead to suboptimal use in practice. In this paper, we propose a move toward a standardized evaluation of imputation methodology. To demonstrate the need for standardization, we highlight a set of possible pitfalls that bring forth a chain of potential problems in the objective assessment of the performance of imputation routines. Additionally, we suggest a course of action for simulating and evaluating missing data problems. Our suggested course of action is by no means meant to serve as a complete cookbook, but rather meant to incite critical thinking and a move to objective and fair evaluations of imputation methodology. We invite the readers of this paper to contribute to the suggested course of action. -->

<!-- We encourage simulators to carefully consider and document their choices in the evaluation of imputation methodology. Simulation studies should not only be well executed, but also properly reported. -->

show simulation figures: all of them validly generated, but only two are valid missingness

# 

## Aims

e.g., simulation scope

- simulation design (incl. pseudocode or flow diagram)
- required level of precision (incl. number of simulation repetitions)

## Data-generating mechanisms

e.g., data generation and missingness generation

- data source (incl. model-based or design-based, sampling variance)
- data characteristics (incl. multivariate relations and data structures such as clustering)
- missingness mechanisms (incl. type or functional form of the missing data model)
- missingness patterns (incl. missingness proportion)


## Estimands

e.g., complete data target

## Methods

e.g., missing data methods, analytic method and inference pooling rules

- imputation methods (incl. parameters such as the number of imputations)
- estimation method (incl. reference methods such as complete case analysis)
- methods used to construct standard errors and confidence intervals
<!-- Such as rules by Barnard and Rubin (1999); Rubin (1987); Reiter (2003), etc. -->

## Performance measures

e.g., evaluation of estimates and imputed values

- statistical properties (incl. comparative performance, if applicable)
- validity of imputations (incl. imputation model fit and distributional characteristics)
<!-- − Does the imputation model fit well to the observed values? (e.g., posterior predictive check) -->
<!-- − Are there unrealistic values in the data? (e.g., negative where positive expected) -->
<!-- − Are there implausible combinations in the data? (e.g., pregnant grandfathers) -->

# Summary

- checklist cf. ADEMP (Morris et al., 2019)



# References

Morris, T. P., White, I. R., & Crowther, M. J. (2019). Using simulation studies to evaluate statistical methods. Statistics in Medicine, 38(11), 2074– 2102.

Oberman, H. I., & Vink, G. (2023). Toward a standardized evaluation of imputation methodology. Biometrical Journal, e2200107. https://doi.org/10.1002/bimj.202200107

Publication archive via [![DOI](https://zenodo.org/badge/449704829.svg)](https://zenodo.org/badge/latestdoi/449704829)

Checklist via [Research Equals](https://doi.org/10.53962/vrd3-8h76)
